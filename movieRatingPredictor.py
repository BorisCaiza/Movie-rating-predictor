# -*- coding: utf-8 -*-
"""ProyectoBorisCaiza.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WeZZocw18bh7BluIr2GUX2Ms_1PC6Nok
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install fancyimpute

import pandas as pd
from pandas import DataFrame
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
import missingno as msno
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.impute import SimpleImputer
from fancyimpute import IterativeImputer as MICE
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.decomposition import PCA
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
import pickle
import joblib
from sklearn.neighbors import KNeighborsRegressor

df = pd.read_csv('./drive/MyDrive/DeberesML/imdb.csv')
df_original = df
df_original.info()

"""#Limpieza"""

sns.set(style="whitegrid", font_scale=1)
plt.figure(figsize=(15,15))
plt.title("Pearson", fontsize = 20)
sns.heatmap(df_original.corr(), vmax= 0.7, square= True, cmap="GnBu", linecolor='b', annot=True, annot_kws= {'size':8} )

"""##Haciendo Nulos y Flotantes"""

df_nuevo = df_original.copy()
df_nuevo["Rate"] = [x.replace("No Rate", "NaN") for x in df_nuevo["Rate"]]
df_nuevo['Rate'] = df_nuevo['Rate'].astype(float, errors = 'raise')
df_nuevo["Votes"] = [x.replace("No Votes", "NaN") for x in df_nuevo["Votes"]]
df_nuevo["Votes"] = [x.replace(",", "") for x in df_nuevo["Votes"]]
df_nuevo['Votes'] = df_nuevo['Votes'].astype(float, errors = 'raise')
df_nuevo["Duration"] = [x.replace("None", "NaN") for x in df_nuevo["Duration"]]
df_nuevo['Duration'] = df_nuevo['Duration'].astype(float, errors = 'raise')
df_nuevo["Episodes"] = [x.replace("-", "NaN") for x in df_nuevo["Episodes"]]
df_nuevo['Episodes'] = df_nuevo['Episodes'].astype(float, errors = 'raise')

df_nuevo.info()

df_nuevo.isnull().sum()

sns.set(style="whitegrid", font_scale=1)
plt.figure(figsize=(15,15))
plt.title("Pearson", fontsize = 20)
sns.heatmap(df_nuevo.corr(), vmax= 0.7, square= True, cmap="GnBu", linecolor='b', annot=True, annot_kws= {'size':8} )

df_nuevo.describe().transpose()

"""##Episodio"""

msno.bar(df_nuevo)

df_nuevo = df_nuevo.drop(['Episodes'], axis = 1)

"""##Genero"""

'''
df_categorizado_genre = df_nuevo.copy()
df_categorizado_genre.Genre = df_categorizado_genre.Genre.str.replace(' ','')
generos = df_categorizado_genre.Genre.str.get_dummies(sep=",")
df_nuevo  = pd.concat([df_nuevo, generos], axis =1, join= 'inner')
df_nuevo = df_nuevo.drop(['Genre'], axis = 1)
df_nuevo
'''

"""##Tipo"""

df_nuevo["Type"] = df_nuevo["Type"].replace("Film", 1)
df_nuevo["Type"] = df_nuevo["Type"].replace("Series", 0)

plt.figure(figsize=(15,15))
sns.countplot(x = 'Type',data=df_nuevo)

"""##Certificado"""

df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-Y', '0')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-G', '0')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('Passed', '0')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('Approved', '0')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('E', '0')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('G', '0')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('PG', '0')



df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-Y7-FV', '0')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-Y7', '0')


df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-14', '1')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('GP', '1')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-14', '1')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-PG', '1')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('M/PG', '1')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('PG-13','1')



df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('NC-17', '2')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('TV-MA', '2')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('M', '2')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('X', '2')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('R','2')


df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('Not Rated', '3')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('Unrated', '3')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('(Banned)', '3')
df_nuevo["Certificate"] = df_nuevo["Certificate"].replace('None', '3')

df_nuevo['Certificate'] = df_nuevo['Certificate'].astype(float, errors = 'raise')

plt.figure(figsize=(15,15))
sns.countplot(x = 'Certificate',data=df_nuevo)

"""##Borrando columnas inesarias"""

df_nuevo = df_nuevo.drop(['Name','Genre'], axis=1)

df_nuevo.info()

"""## Desnudez, Violencia, Profanidad, Alcohol, Atemorizante"""

df_nuevo.replace({'None':0,'Mild':1,'Moderate':2,'Severe':3,'No Rate':0}, inplace = True)

df_nuevo

df_nuevo.info()

sns.set(style="whitegrid", font_scale=1)
plt.figure(figsize=(15,15))
plt.title("Pearson", fontsize = 20)
sns.heatmap(df_nuevo.corr(), vmax= 0.7, square= True, cmap="GnBu", linecolor='b', annot=True, annot_kws= {'size':8} )

df_nuevo.describe().transpose()

df_nuevo = df_nuevo.dropna(axis=0, subset=['Rate'])

df_nuevo

df_nuevo.hist()

def trainTestSplit(X,y):
  X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)
  return X_train,X_test,y_train,y_test

def generarPCA(n_componentes,X):
  pca = PCA(n_components=n_componentes)
  cp = pca.fit_transform(X)
  return pd.DataFrame(data=cp)

X = df_nuevo.drop(['Rate'], axis=1)
y = df_nuevo['Rate']

X

X.isnull().sum()

X = X.fillna(X.mean())

X.isnull().sum()

X.info()

pickle.dump(scaler, open('scaler.sav', 'wb'))

X

X_train,X_test, y_train,y_test = trainTestSplit(X,y)

'''
si = SimpleImputer()
mice = MICE()
#X_train = mice.fit_transform(X_train)
#X_test = mice.fit_transform(X_test)

#X_train = pd.DataFrame(si.fit_transform(X_train))
#X_test = pd.DataFrame(si.fit_transform(X_test))
#X_train = generarPCA(2,X_train)
#X_test = generarPCA(2,X_test)
X_train = X_train.fillna(X_train.mean())
X_test = X_test.fillna(X_test.mean())
scaler = MinMaxScaler()
scaler.fit(X_test)
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
pickle.dump(scaler, open('scaler.sav', 'wb'))
'''

"""#ANN"""

model = Sequential()
model.add(Dense(10, activation='relu'))
model.add(Dense(5, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer = 'adam', loss= 'mse')

model.fit(x=X_train, y = y_train, validation_data=(X_test,y_test),batch_size=80,epochs=50,verbose=1)

loss= pd.DataFrame(model.history.history)
plt.figure(figsize=(15,5))
sns.lineplot(data=loss, lw=3)
plt.xlabel('Epocas')
sns.despine()

r2_score(y_train, model.predict(X_train))

r2_score(y_test, model.predict(X_test))

"""#Random Forest"""

modelRandom = RandomForestRegressor(n_estimators=10, random_state=0)
modelRandom.fit(X_train,y_train)

r2_score(y_train, modelRandom.predict(X_train))

r2_score(y_test, modelRandom.predict(X_test))

pickle.dump(modelRandom, open('random_forest.sav', 'wb'))

mean_absolute_error(y_test, modelRandom.predict(X_test))

mean_absolute_error(y_train, modelRandom.predict(X_train))

"""#Regresion Lineal"""

rl = LinearRegression()
rl.fit(X_train, y_train)

r2_score(y_train, rl.predict(X_train))

r2_score(y_test, rl.predict(X_test))

pickle.dump(rl, open('linear_regression.sav', 'wb'))

mean_absolute_error(y_test, rl.predict(X_test))

mean_absolute_error(y_test, rl.predict(X_test))

"""#SVR"""

svr = SVR(kernel="poly",verbose=True)

svr.fit(X_train,y_train)

svr.score(X_train,y_train)

svr.score(X_test,y_test)

pickle.dump(svr, open('svr.sav', 'wb'))

mean_absolute_error(y_test, svr.predict(X_test))

mean_absolute_error(y_test, svr.predict(X_test))

"""#KNN"""

error = []
for n in range(1,70):
  knn = KNeighborsRegressor(n_neighbors=n)
  knn.fit(X_train, y_train)
  predict_n = knn.predict(X_test)
  error.append(np.mean(predict_n != y_test))
plt.figure(figsize=(15,10))
plt.plot(range(1,70), error, color='b', linestyle = 'dashed', marker = 'o', markerfacecolor = 'r', markersize = '8')

knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(X_train, y_train)

knn.score(X_train, y_train)

knn.score(X_test, y_test)

r2_score(y_train, knn.predict(X_train))

r2_score(y_test, knn.predict(X_test))

mean_absolute_error(y_train, knn.predict(X_train))

mean_absolute_error(y_test, knn.predict(X_test))

pickle.dump(knn, open("knn.sav", 'wb'))

